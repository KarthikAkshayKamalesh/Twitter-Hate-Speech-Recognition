{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ajay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ajay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ajay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.52)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\programdata\\anaconda3\\lib\\site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: pyahocorasick in c:\\programdata\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
      "Requirement already satisfied: anyascii in c:\\programdata\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "#Import required libraries\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "import sys  \n",
    "!{sys.executable} -m pip install contractions\n",
    "import contractions\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>43636</td>\n",
       "      <td>43636</td>\n",
       "      <td>1</td>\n",
       "      <td>If you tell a dyke she looks just like a nigga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6047</td>\n",
       "      <td>6047</td>\n",
       "      <td>0</td>\n",
       "      <td>i am joy. #i_am #positive #affirmation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8555</td>\n",
       "      <td>8555</td>\n",
       "      <td>0</td>\n",
       "      <td>why the more   #actor will #book the #job from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7507</td>\n",
       "      <td>7507</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user @user i know how u feel i didn't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>29933</td>\n",
       "      <td>29933</td>\n",
       "      <td>0</td>\n",
       "      <td>trying not to shut down but maybe #pokemon wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46740</th>\n",
       "      <td>46740</td>\n",
       "      <td>24534</td>\n",
       "      <td>24534</td>\n",
       "      <td>0</td>\n",
       "      <td>you are beautiful girl &amp;amp; choice semoga ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46741</th>\n",
       "      <td>46741</td>\n",
       "      <td>7798</td>\n",
       "      <td>7798</td>\n",
       "      <td>0</td>\n",
       "      <td>@user evergreen college police can't park prop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46742</th>\n",
       "      <td>46742</td>\n",
       "      <td>31654</td>\n",
       "      <td>31654</td>\n",
       "      <td>0</td>\n",
       "      <td>@user we are so   and we just can't hide it! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46743</th>\n",
       "      <td>46743</td>\n",
       "      <td>15989</td>\n",
       "      <td>15989</td>\n",
       "      <td>0</td>\n",
       "      <td>chillax ð´ð´ #pasumpahanisland #island #be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46744</th>\n",
       "      <td>46744</td>\n",
       "      <td>41979</td>\n",
       "      <td>41979</td>\n",
       "      <td>0</td>\n",
       "      <td>I am a Yankee by birth, and lived much of my l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46745 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  label  \\\n",
       "0               0         43636           43636      1   \n",
       "1               1          6047            6047      0   \n",
       "2               2          8555            8555      0   \n",
       "3               3          7507            7507      0   \n",
       "4               4         29933           29933      0   \n",
       "...           ...           ...             ...    ...   \n",
       "46740       46740         24534           24534      0   \n",
       "46741       46741          7798            7798      0   \n",
       "46742       46742         31654           31654      0   \n",
       "46743       46743         15989           15989      0   \n",
       "46744       46744         41979           41979      0   \n",
       "\n",
       "                                                   tweet  \n",
       "0      If you tell a dyke she looks just like a nigga...  \n",
       "1            i am joy. #i_am #positive #affirmation       \n",
       "2      why the more   #actor will #book the #job from...  \n",
       "3       @user @user @user i know how u feel i didn't ...  \n",
       "4      trying not to shut down but maybe #pokemon wil...  \n",
       "...                                                  ...  \n",
       "46740  you are beautiful girl &amp; choice semoga ten...  \n",
       "46741  @user evergreen college police can't park prop...  \n",
       "46742   @user we are so   and we just can't hide it! ...  \n",
       "46743  chillax ð´ð´ #pasumpahanisland #island #be...  \n",
       "46744  I am a Yankee by birth, and lived much of my l...  \n",
       "\n",
       "[46745 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting Data from the .csv file\n",
    "\n",
    "\n",
    "# Use the the path of the .csv file, that I have attached along with the code\n",
    "tweets_train_dataset=pd.read_csv(\"C:/Users/Ajay/Desktop/Hatespeech/FinalDataset_Train1.csv\")\n",
    "# tweets_train_dataset\n",
    "tweets_test_dataset=pd.read_csv(\"C:/Users/Ajay/Desktop/Hatespeech/FinalDataset_Test1.csv\")\n",
    "tweets_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tell',\n",
       " 'dyke',\n",
       " 'looks',\n",
       " 'like',\n",
       " 'nigga',\n",
       " 'compliment',\n",
       " 'always',\n",
       " 'wondered',\n",
       " 'joy',\n",
       " 'positive',\n",
       " 'affirmation',\n",
       " 'actor',\n",
       " 'book',\n",
       " 'job',\n",
       " 'user',\n",
       " 'know',\n",
       " 'feel',\n",
       " 'seen',\n",
       " 'jocoxmp',\n",
       " 'bbc',\n",
       " 'parliament',\n",
       " 'times',\n",
       " 'still',\n",
       " 'trying',\n",
       " 'shut',\n",
       " 'maybe',\n",
       " 'pokemon',\n",
       " 'help',\n",
       " '90skid',\n",
       " 'bihday',\n",
       " 'tomorrow',\n",
       " 'june',\n",
       " '19',\n",
       " 'crhedrys',\n",
       " 'pussy',\n",
       " 'licking',\n",
       " 'meow',\n",
       " 'stopwhitepeople2014',\n",
       " '128049',\n",
       " 'https',\n",
       " 'co',\n",
       " 'keegdcjs5k',\n",
       " '8221',\n",
       " '128533',\n",
       " 'chanelisabeth',\n",
       " 'nigger',\n",
       " 'music',\n",
       " 'nearly',\n",
       " 'folks',\n",
       " 'nervous',\n",
       " 'curious',\n",
       " 'finalised',\n",
       " 'stuff',\n",
       " 'pointers',\n",
       " 'grainne',\n",
       " 'bringiton',\n",
       " 'rt',\n",
       " 'faz',\n",
       " 'convinced',\n",
       " 'hitler',\n",
       " 'faggit',\n",
       " 'bitches',\n",
       " 'kids',\n",
       " 'leather',\n",
       " 'uniforms',\n",
       " 'wanted',\n",
       " 'alone',\n",
       " 'closest',\n",
       " 'men',\n",
       " 'lock',\n",
       " '8230',\n",
       " 'lit',\n",
       " 'fire',\n",
       " 'cheer',\n",
       " 'cold',\n",
       " 'damn',\n",
       " 'girl',\n",
       " 'porch',\n",
       " 'monkey',\n",
       " 'smh',\n",
       " 'sad',\n",
       " 'real',\n",
       " 'got',\n",
       " 'find',\n",
       " 'negative',\n",
       " 'lbj',\n",
       " 'made',\n",
       " 'history',\n",
       " '3',\n",
       " '1',\n",
       " 'bk',\n",
       " '2',\n",
       " '41s',\n",
       " 'triple',\n",
       " 'double',\n",
       " 'zone',\n",
       " 'secondary',\n",
       " 'lie',\n",
       " 'bitch',\n",
       " 'flier',\n",
       " 'pet',\n",
       " 'canary',\n",
       " 'spirited',\n",
       " 'work',\n",
       " 'super',\n",
       " 'early',\n",
       " 'nowhere',\n",
       " 'get',\n",
       " 'drink',\n",
       " 'since',\n",
       " 'late',\n",
       " 'favorite',\n",
       " 'show',\n",
       " 'live',\n",
       " 'finales',\n",
       " 'painful',\n",
       " 'watch',\n",
       " 'one',\n",
       " 'hear',\n",
       " 'amp',\n",
       " 'awkward',\n",
       " 'ariellesloth',\n",
       " 'na',\n",
       " 'snapshot',\n",
       " 'kinky',\n",
       " 'horny',\n",
       " 'naughty',\n",
       " 'xxx',\n",
       " 'hot',\n",
       " 'slut',\n",
       " 'sexy',\n",
       " 'wet',\n",
       " 'shy',\n",
       " 'nasty',\n",
       " 'nude',\n",
       " 'young',\n",
       " 'porn',\n",
       " 'wow',\n",
       " 'first',\n",
       " 'lamebrain',\n",
       " 'block',\n",
       " 'twitter',\n",
       " 'chick',\n",
       " 'would',\n",
       " 'commit',\n",
       " 'rest',\n",
       " 'gay',\n",
       " 'life',\n",
       " 'complete',\n",
       " '100',\n",
       " 'submissive',\n",
       " 'bottom',\n",
       " 'could',\n",
       " 'zac',\n",
       " 'efron',\n",
       " 'noshame',\n",
       " 'gayboyproblems',\n",
       " 'goldiggers',\n",
       " 'want',\n",
       " 'marry',\n",
       " 'somebody',\n",
       " 'hoes',\n",
       " 'see',\n",
       " 'therapy',\n",
       " '8220',\n",
       " 'pot',\n",
       " 'headed',\n",
       " 'ready',\n",
       " 'another',\n",
       " 'tat',\n",
       " 'doeeee',\n",
       " '128527',\n",
       " 'right',\n",
       " 'hoe',\n",
       " 'fear',\n",
       " 'niggah',\n",
       " 'trust',\n",
       " 'bitchhh',\n",
       " 'roadtrip',\n",
       " 'day',\n",
       " '13hrs',\n",
       " 'little',\n",
       " 'rock',\n",
       " 'summer',\n",
       " 'adventure',\n",
       " 'selfie',\n",
       " 'ludicrous',\n",
       " '2think',\n",
       " 'happening',\n",
       " 'especially',\n",
       " 'come',\n",
       " 'far',\n",
       " 'taken',\n",
       " '2leaps',\n",
       " 'backwards',\n",
       " 'herfarm',\n",
       " 'steveworks4you',\n",
       " 'thedemocrats',\n",
       " 'low',\n",
       " 'info',\n",
       " 'redneck',\n",
       " 'dumbasses',\n",
       " 'vote',\n",
       " 'corporate',\n",
       " 'shills',\n",
       " 'idiots',\n",
       " 'believe',\n",
       " 'dummies',\n",
       " 'jayswaggkillah',\n",
       " 'jackies',\n",
       " 'retard',\n",
       " 'blondeproblems',\n",
       " 'least',\n",
       " 'make',\n",
       " 'grilled',\n",
       " 'cheese',\n",
       " 'try',\n",
       " 'rex',\n",
       " 'new',\n",
       " 'water',\n",
       " 'roasted',\n",
       " 'tomato',\n",
       " 'potato',\n",
       " 'sawdust',\n",
       " 'crackers',\n",
       " 'loaded',\n",
       " 'enough',\n",
       " 'sodium',\n",
       " 'lots',\n",
       " 'wife',\n",
       " 'look',\n",
       " 'unsavory',\n",
       " '22',\n",
       " '99',\n",
       " 'box',\n",
       " 'dizzybala',\n",
       " 'man',\n",
       " 'around',\n",
       " 'lot',\n",
       " 'bunch',\n",
       " 'ambriel',\n",
       " 'getting',\n",
       " 'us',\n",
       " 'set',\n",
       " 'soon',\n",
       " 'waiting',\n",
       " 'missknf',\n",
       " 'read',\n",
       " 'playing',\n",
       " 'lutsen',\n",
       " 'mn',\n",
       " '9',\n",
       " '00',\n",
       " 'pm',\n",
       " 'today',\n",
       " 'papa',\n",
       " 'charlies',\n",
       " 'http',\n",
       " 'artistdata',\n",
       " 'com',\n",
       " '2oy4',\n",
       " 'singing',\n",
       " 'worship',\n",
       " 'team',\n",
       " 'elgin',\n",
       " 'city',\n",
       " 'church',\n",
       " 'sammi',\n",
       " 'boyden',\n",
       " 'idk',\n",
       " 'let',\n",
       " 'shit',\n",
       " 'ask',\n",
       " 'samuela',\n",
       " 'trump',\n",
       " 'brexit',\n",
       " 'based',\n",
       " 'misconceptions',\n",
       " 'race',\n",
       " 'nationality',\n",
       " 'says',\n",
       " 'renowned',\n",
       " 'philosopher',\n",
       " 'cbc',\n",
       " 'radio',\n",
       " 'celebration',\n",
       " 'continued',\n",
       " 'office',\n",
       " 'weeklongcelebration',\n",
       " 'debscooking',\n",
       " 'swim',\n",
       " 'swimming',\n",
       " 'sun',\n",
       " 'sunny',\n",
       " 'sea',\n",
       " 'seaside',\n",
       " 'colorized',\n",
       " 'w',\n",
       " 'rainbow',\n",
       " 'danian',\n",
       " 'love',\n",
       " 'colour',\n",
       " 'depok',\n",
       " 'dad',\n",
       " 'person',\n",
       " 'loving',\n",
       " 'kind',\n",
       " 'thank',\n",
       " 'fathersday',\n",
       " 'taylorlopez96',\n",
       " 'harleighpurolla',\n",
       " 'biggest',\n",
       " 'two',\n",
       " 'faced',\n",
       " 'ever',\n",
       " 'met',\n",
       " 'need',\n",
       " 'jump',\n",
       " 'shop',\n",
       " 'fritag',\n",
       " '72',\n",
       " 'beers',\n",
       " 'hate',\n",
       " 'looking',\n",
       " '20',\n",
       " 'odd',\n",
       " '120',\n",
       " 'fags',\n",
       " 'tae',\n",
       " 'going',\n",
       " 'junkie',\n",
       " 'enjoy',\n",
       " 'weekend',\n",
       " 'friends',\n",
       " 'family',\n",
       " 'nearest',\n",
       " 'outlet',\n",
       " 'weazy',\n",
       " 'birdman',\n",
       " 'jr',\n",
       " 'wxnx94bzoc',\n",
       " 'simple',\n",
       " 'happiness',\n",
       " 'grateful',\n",
       " 'mother',\n",
       " 'wonderwomanshop',\n",
       " 'wishes',\n",
       " 'sabbathday',\n",
       " 'parents',\n",
       " 'candacevon',\n",
       " 'niggas',\n",
       " 'talk',\n",
       " 'days',\n",
       " '128530',\n",
       " 'facts',\n",
       " '5',\n",
       " 'lt',\n",
       " '33333',\n",
       " 'oitnbseason4',\n",
       " 'cantwait',\n",
       " 'identity',\n",
       " 'disorder',\n",
       " 'social',\n",
       " 'rejection',\n",
       " 'many',\n",
       " 'libs',\n",
       " 'never',\n",
       " 'much',\n",
       " 'feminism',\n",
       " 'fuelled',\n",
       " 'anger',\n",
       " 'recently',\n",
       " 'socialjustice',\n",
       " 'thought',\n",
       " 'london',\n",
       " 'year',\n",
       " 'begins',\n",
       " 'uk',\n",
       " 'straight',\n",
       " 'bruh',\n",
       " 'stop',\n",
       " 'fuckin',\n",
       " 'joebudden',\n",
       " '8217',\n",
       " 'cuz',\n",
       " 'attainable',\n",
       " 'bro',\n",
       " 'pink',\n",
       " 'barber',\n",
       " 'branding',\n",
       " 'packaging',\n",
       " 'design',\n",
       " 'creative',\n",
       " 'fun',\n",
       " 'gayathomedad',\n",
       " 'somewhere',\n",
       " 'jerking',\n",
       " 'hand',\n",
       " 'lose',\n",
       " 'firework',\n",
       " 'accident',\n",
       " 'thanks',\n",
       " 'helping',\n",
       " 'customer',\n",
       " 'c',\n",
       " 'later',\n",
       " 'guyz',\n",
       " 'bad',\n",
       " 'vibez',\n",
       " 'please',\n",
       " 'feeling',\n",
       " 'wonderful',\n",
       " 'attitude',\n",
       " 'slutty',\n",
       " 'hermione',\n",
       " 'twerking',\n",
       " 'chamber',\n",
       " 'secrets',\n",
       " 'twerkteam',\n",
       " 'wheels',\n",
       " 'turning',\n",
       " 'things',\n",
       " 'beyond',\n",
       " 'excited',\n",
       " 'dreamsarecomingtrue',\n",
       " 'onedirectum',\n",
       " 'fugly',\n",
       " 'white',\n",
       " 'mikediggem',\n",
       " 'yo',\n",
       " 'fuck',\n",
       " 'broke',\n",
       " 'really',\n",
       " 'niggs',\n",
       " 'g',\n",
       " '2016',\n",
       " 'expressed',\n",
       " 'photo',\n",
       " 'euref',\n",
       " 'johnfaheyiii',\n",
       " 'go',\n",
       " 'eat',\n",
       " 'bear',\n",
       " 'yokel',\n",
       " 'agree',\n",
       " 'policies',\n",
       " 'suppo',\n",
       " 'anyway',\n",
       " 'pathetic',\n",
       " 'makeup',\n",
       " 'trash',\n",
       " 'distance',\n",
       " 'anything',\n",
       " 'useful',\n",
       " 'inspiring',\n",
       " 'funny',\n",
       " 'joyful',\n",
       " 'augusta',\n",
       " 'buffalo',\n",
       " 'simulation',\n",
       " 'take',\n",
       " 'vicinity',\n",
       " 'homes',\n",
       " 'way',\n",
       " 'springfield',\n",
       " 'bull',\n",
       " 'dominate',\n",
       " 'direct',\n",
       " 'whatever',\n",
       " 'date',\n",
       " 'wit',\n",
       " 'irene',\n",
       " 'nd',\n",
       " 'stood',\n",
       " 'matter',\n",
       " 'fact',\n",
       " 'da',\n",
       " 'whole',\n",
       " 'thanx',\n",
       " 'fucking',\n",
       " 'cunt',\n",
       " 'ur',\n",
       " 'ass',\n",
       " 'grease',\n",
       " 'false',\n",
       " 'lashes',\n",
       " 'delivered',\n",
       " 'wait',\n",
       " 'hair',\n",
       " 'extensions',\n",
       " 'night',\n",
       " 'town',\n",
       " 'order',\n",
       " '13',\n",
       " 'til',\n",
       " 'glasto',\n",
       " 'sta',\n",
       " 'preparing',\n",
       " 'model',\n",
       " 'time',\n",
       " 'greg',\n",
       " 'jon3s',\n",
       " 'hope',\n",
       " 'future',\n",
       " 'might',\n",
       " 'sleeping',\n",
       " 'studying',\n",
       " 'instead',\n",
       " 'shutcho',\n",
       " 'asss',\n",
       " 'welcome',\n",
       " 'already',\n",
       " 'introduced',\n",
       " 'paner',\n",
       " 'moso2016',\n",
       " 'say',\n",
       " 'bird',\n",
       " 'gets',\n",
       " 'worm',\n",
       " 'puts',\n",
       " 'gummy',\n",
       " 'worms',\n",
       " 'morning',\n",
       " 'coffee',\n",
       " 'agreed',\n",
       " 'sunday',\n",
       " 'relax',\n",
       " 'proverb',\n",
       " 'breathetherapies',\n",
       " 'nobody',\n",
       " 'else',\n",
       " 'texting',\n",
       " 'inboxing',\n",
       " 'kiking',\n",
       " 'dming',\n",
       " 'snapchatting',\n",
       " 'emailing',\n",
       " 'charlesbwaffle',\n",
       " 'oh',\n",
       " 'cool',\n",
       " 'though',\n",
       " 'brownie',\n",
       " 'lol',\n",
       " 'keep',\n",
       " 'bees',\n",
       " 'naturallawn',\n",
       " 'clover',\n",
       " 'igers',\n",
       " 'instadaily',\n",
       " 'purchase',\n",
       " 'proscar',\n",
       " 'buy',\n",
       " 'without',\n",
       " 'prescription',\n",
       " 'books',\n",
       " 'beautiful',\n",
       " 'feels',\n",
       " 'coverlove',\n",
       " 'bookporn',\n",
       " 'mom',\n",
       " 'careless',\n",
       " 'prayforoakland',\n",
       " 'tag',\n",
       " 'full',\n",
       " 'f',\n",
       " 'libtard',\n",
       " 'bot',\n",
       " 'comments',\n",
       " 'trolled',\n",
       " 'awhile',\n",
       " 'repetitive',\n",
       " 'word4word',\n",
       " 'chicago',\n",
       " 'taking',\n",
       " 'move',\n",
       " 'house',\n",
       " 'returning',\n",
       " 'logo',\n",
       " 'logodesign',\n",
       " 'websitedesigner',\n",
       " 'persistence',\n",
       " 'paid',\n",
       " '6',\n",
       " 'months',\n",
       " 'finally',\n",
       " 'internet',\n",
       " 'homestead',\n",
       " '2014',\n",
       " 'rather',\n",
       " 'jesus',\n",
       " 'silver',\n",
       " 'gold',\n",
       " 'bet',\n",
       " 'hates',\n",
       " 'omfgmeezy',\n",
       " 'stole',\n",
       " 'niglet',\n",
       " 'rakwonogod',\n",
       " 'lmaoo',\n",
       " '2dhlgxcvg2',\n",
       " 'itsfoodporn',\n",
       " 'oreo',\n",
       " 'cheesecake',\n",
       " 'bars',\n",
       " 'rlhhim98fl',\n",
       " 'great',\n",
       " 'end',\n",
       " 'happy',\n",
       " 'everyone',\n",
       " 'luxurytravel',\n",
       " 'travel',\n",
       " 'saturdaynight',\n",
       " 'wanderlust',\n",
       " 'nah',\n",
       " 'bitter',\n",
       " 'heat',\n",
       " 'yeiiowbang',\n",
       " 'seeing',\n",
       " 'bulls',\n",
       " 'fans',\n",
       " 'along',\n",
       " 'hardcore',\n",
       " 'sets',\n",
       " 'nakedcurvygirl',\n",
       " 'pics',\n",
       " 'drakeandjosh',\n",
       " 'wa',\n",
       " 'bitterchick',\n",
       " 'hellobaphomet',\n",
       " 'crazy',\n",
       " 'wants',\n",
       " 'lay',\n",
       " 'bowf',\n",
       " 'use',\n",
       " 'power',\n",
       " 'mind',\n",
       " 'heal',\n",
       " 'body',\n",
       " 'altwaystoheal',\n",
       " 'healthy',\n",
       " 'friday',\n",
       " 'shoulder',\n",
       " 'workout',\n",
       " 'soccer',\n",
       " 'game',\n",
       " 'gd',\n",
       " 'gdragon',\n",
       " 'kwonjiyong',\n",
       " 'jiyong',\n",
       " 'vip',\n",
       " 'bigbang',\n",
       " 'handsome',\n",
       " 'cute',\n",
       " 'allahsoil',\n",
       " 'people',\n",
       " 'originally',\n",
       " 'meant',\n",
       " 'landholding',\n",
       " 'males',\n",
       " 'mc',\n",
       " 'phillysuppophilly',\n",
       " 'combat',\n",
       " 'msnbc',\n",
       " 'cnn',\n",
       " 'amjoy',\n",
       " 'joe',\n",
       " 'biden',\n",
       " 'unmasking',\n",
       " 'america',\n",
       " 'gucci',\n",
       " 'cut',\n",
       " 'throat',\n",
       " 'stay',\n",
       " 'next',\n",
       " 'door',\n",
       " 'kenfolk',\n",
       " 'limousine',\n",
       " 'pouring',\n",
       " 'ya',\n",
       " 'rent',\n",
       " 'money',\n",
       " 'wondrous',\n",
       " 'justdoit',\n",
       " 'rob',\n",
       " 'throw',\n",
       " 'cheeseburger',\n",
       " 'brosconfessions',\n",
       " 'unlike',\n",
       " 'bros',\n",
       " 'actually',\n",
       " 'care',\n",
       " 'called',\n",
       " 'tool',\n",
       " 'healing',\n",
       " 'yrnreggie',\n",
       " '128529',\n",
       " '128514',\n",
       " 'fcqfdki4zn',\n",
       " 'germany',\n",
       " 'warm',\n",
       " 'notacloudinthesky',\n",
       " 'best',\n",
       " 'share',\n",
       " 'crayons',\n",
       " 'color',\n",
       " 'together',\n",
       " 'friendship',\n",
       " 'week',\n",
       " 'omg',\n",
       " 'uganda',\n",
       " 'scared',\n",
       " 'donaldtrump',\n",
       " 'nothing',\n",
       " 'good',\n",
       " 'racist',\n",
       " 'misogynist',\n",
       " 'women',\n",
       " 'animals',\n",
       " 'tynodollasign',\n",
       " 'talkin',\n",
       " 'owe',\n",
       " 'fym',\n",
       " '8',\n",
       " 'yrs',\n",
       " 'endless',\n",
       " 'lies',\n",
       " 'contempt',\n",
       " 'left',\n",
       " 'activist',\n",
       " 'realize',\n",
       " 'clarity',\n",
       " 'came',\n",
       " 'clean',\n",
       " 'neomd71',\n",
       " 'bizarre',\n",
       " 'sports',\n",
       " 'cle',\n",
       " 'browns',\n",
       " 'win',\n",
       " 'felt',\n",
       " 'loss',\n",
       " 'failed',\n",
       " 'ex',\n",
       " 'qb',\n",
       " 'duel',\n",
       " 'cavs',\n",
       " 'ho',\n",
       " 'xianilee',\n",
       " 'boom',\n",
       " 'cancerous',\n",
       " 'kevin',\n",
       " 'timrosscomedy',\n",
       " 'michael',\n",
       " 'craig12',\n",
       " 'tim',\n",
       " 'ross',\n",
       " 'cowboys',\n",
       " 'nfc',\n",
       " 'faggots',\n",
       " 'inhalelove',\n",
       " 'alex',\n",
       " 'target',\n",
       " 'angry',\n",
       " 'tonight',\n",
       " 'dis',\n",
       " 'tweeter',\n",
       " 'hard',\n",
       " 'colored',\n",
       " 'bein',\n",
       " 'waffle',\n",
       " 'mississippi',\n",
       " 'acts',\n",
       " 'humans',\n",
       " 'even',\n",
       " 'shock',\n",
       " 'anymore',\n",
       " 'tragic',\n",
       " 'drop',\n",
       " 'guns',\n",
       " 'ffs',\n",
       " 'ripchristina',\n",
       " 'lovely',\n",
       " 'hanging',\n",
       " 'baskets',\n",
       " 'beaminster',\n",
       " 'lovedorset',\n",
       " 'local',\n",
       " 'pub',\n",
       " 'private',\n",
       " 'tracks',\n",
       " 'public',\n",
       " 'soundcloud',\n",
       " 'needa',\n",
       " 'finish',\n",
       " 'joints',\n",
       " 'sick',\n",
       " 'clap',\n",
       " 'room',\n",
       " 'roof',\n",
       " '4',\n",
       " 'bottle',\n",
       " 'dispenser',\n",
       " 'stops',\n",
       " 'working',\n",
       " 'lunarorossa',\n",
       " '100ml',\n",
       " 'figured',\n",
       " 'pregnant',\n",
       " 'probably',\n",
       " 'loneliest',\n",
       " 'pregnancyproblems',\n",
       " 'baby',\n",
       " 'moose',\n",
       " 'plays',\n",
       " 'backyard',\n",
       " 'sprinkler',\n",
       " 'check',\n",
       " 'blog',\n",
       " 'beachday',\n",
       " 'pool',\n",
       " 'cleavage',\n",
       " 'lgbt',\n",
       " 'igdaily',\n",
       " 'cam',\n",
       " 'actual',\n",
       " 'shootings',\n",
       " 'attack',\n",
       " 'hu',\n",
       " 'prayfororlando',\n",
       " 'speechless',\n",
       " 'mark',\n",
       " 'view',\n",
       " 'prague',\n",
       " 'czech',\n",
       " 'europe',\n",
       " 'tr',\n",
       " 'friendly',\n",
       " 'born',\n",
       " 'third',\n",
       " 'base',\n",
       " 'hit',\n",
       " 'officials',\n",
       " 'watching',\n",
       " 'extend',\n",
       " 'fought',\n",
       " 'sides',\n",
       " 'argument',\n",
       " 'yesterday',\n",
       " 'itsdorry',\n",
       " 'school',\n",
       " 'test',\n",
       " 'intelligence',\n",
       " 'tests',\n",
       " 'memory',\n",
       " 'ability',\n",
       " 'calm',\n",
       " 'slap',\n",
       " 'maniac3x',\n",
       " 'buck',\n",
       " 'beaners',\n",
       " 'security',\n",
       " 'started',\n",
       " 'pushing',\n",
       " 'jetsgreen',\n",
       " 'faggot',\n",
       " 'release',\n",
       " 'line',\n",
       " 'meyakardashian',\n",
       " 'insane',\n",
       " 'twiterdeze',\n",
       " 'nuts',\n",
       " 'sentence',\n",
       " 'describes',\n",
       " 'words',\n",
       " 'well',\n",
       " 'haha',\n",
       " 'laydeesweets',\n",
       " 'think',\n",
       " 'confused',\n",
       " 'theyaintknow',\n",
       " 'vivalajuicyy69',\n",
       " 'buckm00se',\n",
       " 'remember',\n",
       " 'fat',\n",
       " 'stealing',\n",
       " 'kfc',\n",
       " 'bucket',\n",
       " 'hauling',\n",
       " 'campos',\n",
       " 'uli',\n",
       " 'fag',\n",
       " 'hesgay',\n",
       " 'phone',\n",
       " 'basic',\n",
       " 'girly',\n",
       " 'dry',\n",
       " 'n',\n",
       " 'plain',\n",
       " '128553',\n",
       " 'lnsanetweets',\n",
       " 'girls',\n",
       " 'wear',\n",
       " 'beanies',\n",
       " 'stylish',\n",
       " 'member',\n",
       " 'drug',\n",
       " 'cartel',\n",
       " 'sel',\n",
       " 'xspkccaypi',\n",
       " 'yes',\n",
       " 'daughters',\n",
       " 'official',\n",
       " 'arseholes',\n",
       " 'eu',\n",
       " 'brits',\n",
       " 'leave',\n",
       " 'euro2016',\n",
       " '128171',\n",
       " 'yay',\n",
       " 'except',\n",
       " 'ellen',\n",
       " 'suppoing',\n",
       " 'kimburrell',\n",
       " 'homo',\n",
       " 'resistant',\n",
       " 'storytime',\n",
       " 'wheel',\n",
       " 'thirdwheel',\n",
       " 'awkwardmoment',\n",
       " 'hilarious',\n",
       " 'storytelling',\n",
       " 'youtube',\n",
       " 'successful',\n",
       " 'professional',\n",
       " 'grad',\n",
       " 'student',\n",
       " 'likeagirl',\n",
       " 'nekkohbk',\n",
       " 'dykes',\n",
       " 'dating',\n",
       " 'scene',\n",
       " 'wack',\n",
       " 'dog',\n",
       " 'overreact',\n",
       " 'mr',\n",
       " 'ignore',\n",
       " 'areas',\n",
       " 'listen',\n",
       " 'expect',\n",
       " 'dewayneee',\n",
       " 'outside',\n",
       " 'telling',\n",
       " 'business',\n",
       " 'heyshelby',\n",
       " 'griiind',\n",
       " 'thug',\n",
       " 'luuuuuuv',\n",
       " 'hello',\n",
       " 'maggystylerd',\n",
       " 'rd',\n",
       " 'lifestyle',\n",
       " 'lifestyleblogger',\n",
       " 'like4like',\n",
       " 'likeforlike',\n",
       " 'aipo',\n",
       " 'mountaineering',\n",
       " 'wh',\n",
       " 'handmade',\n",
       " 'etsy',\n",
       " 'mensfashion',\n",
       " 'highfashion',\n",
       " 'dope',\n",
       " 'bracelets',\n",
       " 'lion',\n",
       " 'todays',\n",
       " '16',\n",
       " '06',\n",
       " 'hours',\n",
       " 'spiritshapers',\n",
       " 'announcement',\n",
       " 'fancy',\n",
       " 'gs16',\n",
       " 'xdsmooth',\n",
       " 'took',\n",
       " '1bookieg',\n",
       " 'bsails33',\n",
       " 'ron',\n",
       " '127752',\n",
       " '127917',\n",
       " 'pic',\n",
       " 'flossin',\n",
       " 'woodall',\n",
       " 'fatstinker',\n",
       " 'israel',\n",
       " 'rape',\n",
       " 'land',\n",
       " 'freedom',\n",
       " 'palestine',\n",
       " 'dhugga',\n",
       " 'h',\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-processing the data\n",
    "\n",
    "# Stemming and Lemmatization is usually done in pre-processing\n",
    "# to get better results. But in our our case, these processes affects the result negatively.\n",
    "# So these are not used (Code commented out)\n",
    "\n",
    "Bag_of_Words=[]\n",
    "for Key,Value in tweets_train_dataset.iterrows():\n",
    "    words = Value['tweet']\n",
    "    \n",
    "# Removing Contractions: Contractions are words or combinations of words that are shortened by \n",
    "# dropping letters and replacing them by an apostrophe. These shortened words reverted back to their actual forms.\n",
    "# e.g. don't -> do not , I'm -> I am.\n",
    "    expanded_words = []    \n",
    "    for word in words.split():\n",
    "      # using contractions.fix to expand the shotened words\n",
    "      expanded_words.append(contractions.fix(word))   \n",
    "    words = ' '.join(expanded_words)\n",
    "#     print(words)\n",
    "\n",
    "# Removing special characters from the words\n",
    "# e.g. vs. -> vs , sub-layers -> sub layers \n",
    "    words = re.sub(r'[^A-Za-z0-9 ]+',' ',words)\n",
    "\n",
    "# split into words\n",
    "    words=word_tokenize(words)\n",
    "    \n",
    "# convert to lower case\n",
    "    words = [w.lower() for w in words]\n",
    "    \n",
    "# Remove stop words\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    words = [i for i in words if not i in stop_words]\n",
    "    \n",
    "# Stemming: Stemming is the process of producing morphological variants of a root/base word.\n",
    "# e.g. words: liking, likes, liked , likely ->  root word: like\n",
    "#     ps = PorterStemmer()\n",
    "#     words = [ps.stem(word) for word in words]\n",
    "\n",
    "# Lemmatization: Lemmatization is the process of grouping together the different inflected forms\n",
    "# of a word so they can be analysed as a single item. Lemmatization is \n",
    "# similar to stemming but it brings context to the words. So it links words with similar meaning to one word.\n",
    "# e.g. rocks -> rock, corpora -> corpus.\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "#     print(words)\n",
    "    for k in words:\n",
    "        if k not in Bag_of_Words:\n",
    "            Bag_of_Words.append(k)\n",
    "Bag_of_Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Words: 56733\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of Words:\",len(Bag_of_Words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization using Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization using Count Vectorizer\n",
    "Vector_tweets= CountVectorizer()\n",
    "Vector_tweets = Vector_tweets.fit(Bag_of_Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train data\n",
    "X = tweets_train_dataset['tweet']\n",
    "Y = tweets_train_dataset['label']\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to implement Machine learning models\n",
    "# If train_accuracy is 1 then method is called to compute training accuracy, else testing accuracy\n",
    "def Machine_Learning_Models(X_train,y_train,X_test,y_test,train_accuracy):\n",
    "    Final_pred= Vector_tweets.transform(X_train)\n",
    "    Final_pred\n",
    "    Accuracies_of_All_methods = []\n",
    "    # K-Nearest Neighbor Classifier\n",
    "    Knn = KNeighborsClassifier(n_neighbors=1)\n",
    "    Knn.fit(Final_pred,y_train)\n",
    "    X_test = Vector_tweets.transform(X_test)\n",
    "    prediction = Knn.predict(X_test)\n",
    "    Accuracies_of_All_methods.append(accuracy_score(y_test,prediction))\n",
    "    if train_accuracy == 1:\n",
    "        print(\"K-Nearest Neighbor Classifier done\")\n",
    "    else:\n",
    "        print(\"K-Nearest Neighbor Classifier :-\")\n",
    "        print(\"Confusion Matrix\")\n",
    "        print(confusion_matrix(y_test,prediction))\n",
    "        print(\"Classification Report\")\n",
    "        print(classification_report(y_test,prediction))\n",
    "    \n",
    "    # Support Vector Machine Classifier\n",
    "    Support_Vector_Machine = SVC()\n",
    "    Support_Vector_Machine.fit(Final_pred,y_train)\n",
    "    prediction = Support_Vector_Machine.predict(X_test)\n",
    "    Accuracies_of_All_methods.append(accuracy_score(y_test,prediction))\n",
    "    if train_accuracy == 1:\n",
    "        print(\"Support Vector Machine Classifier done\")\n",
    "    else:\n",
    "        print(\"Support Vector Machine Classifier :-\")\n",
    "        print(\"Confusion Matrix\")\n",
    "        print(confusion_matrix(y_test,prediction))\n",
    "        print(\"Classification Report\")\n",
    "        print(classification_report(y_test,prediction))\n",
    "    \n",
    "    # Multinomial Naive Bayes classifier\n",
    "    Multinomial_Naive_Bayes = MultinomialNB()\n",
    "    Multinomial_Naive_Bayes.fit(Final_pred,y_train)\n",
    "    prediction = Multinomial_Naive_Bayes.predict(X_test)\n",
    "    Accuracies_of_All_methods.append(accuracy_score(y_test,prediction))\n",
    "    if train_accuracy == 1:\n",
    "        print(\"Multinomial Naive Bayes classifier done\")\n",
    "    else:\n",
    "        print(\"Multinomial Naive Bayes classifier :-\")\n",
    "        print(\"Confusion Matrix\")\n",
    "        print(confusion_matrix(y_test,prediction))\n",
    "        print(\"Classification Report\")\n",
    "        print(classification_report(y_test,prediction))\n",
    "    \n",
    "    # Logistic Regression Classifier\n",
    "    Logistic_Regression = LogisticRegression()\n",
    "    Logistic_Regression.fit(Final_pred,y_train)\n",
    "    prediction = Logistic_Regression.predict(X_test)\n",
    "    Accuracies_of_All_methods.append(accuracy_score(y_test,prediction))\n",
    "    if train_accuracy == 1:\n",
    "        print(\"Logistic Regression Classifier done\")\n",
    "        return Accuracies_of_All_methods\n",
    "    else:\n",
    "        print(\"Logistic Regression Classifier :-\")\n",
    "        print(\"Confusion Matrix\")\n",
    "        print(confusion_matrix(y_test,prediction))\n",
    "        print(\"Classification Report\")\n",
    "        print(classification_report(y_test,prediction))\n",
    "    print(\"Test Accuracy of Models:-\")\n",
    "    print(\"KNN : \",Accuracies_of_All_methods[0])\n",
    "    print(\"SVM : \",Accuracies_of_All_methods[1])\n",
    "    print(\"Naive Bayes : \",Accuracies_of_All_methods[2])\n",
    "    print(\"Logistic Regression : \",Accuracies_of_All_methods[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using 5 Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold1\n",
      "K-Nearest Neighbor Classifier done\n",
      "Support Vector Machine Classifier done\n",
      "Multinomial Naive Bayes classifier done\n",
      "Logistic Regression Classifier done\n",
      "[0.8154882875173816, 0.9423467750561557, 0.918921809819232, 0.9452347844689272]\n"
     ]
    }
   ],
   "source": [
    "# 5 Fold cross validation is implemented to find testing accuracy\n",
    "\n",
    "# Fold 1\n",
    "print(\"Fold1\")\n",
    "X_train1 = X[9349:]\n",
    "Y_train1 = Y[9349:]\n",
    "X_val1 = X[:9349]\n",
    "Y_val1 = Y[:9349]\n",
    "\n",
    "fold1 = Machine_Learning_Models(X_train1, Y_train1, X_val1, Y_val1,1)\n",
    "print(fold1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2\n",
      "K-Nearest Neighbor Classifier done\n",
      "Support Vector Machine Classifier done\n",
      "Multinomial Naive Bayes classifier done\n",
      "Logistic Regression Classifier done\n",
      "[0.8879024494598353, 0.9399935822013049, 0.9172103968338859, 0.9422398117445716]\n"
     ]
    }
   ],
   "source": [
    "# Fold2\n",
    "print(\"Fold 2\")\n",
    "fold2= []\n",
    "X_train2 = np.concatenate((X[:9349], X[18698:]), axis=0)\n",
    "Y_train2 = np.concatenate((Y[:9349], Y[18698:]), axis=0)\n",
    "X_val2 = X[9349:18698]\n",
    "Y_val2 = Y[9349:18698]\n",
    "\n",
    "fold2 = Machine_Learning_Models(X_train2, Y_train2, X_val2, Y_val2,1)\n",
    "print(fold2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3\n",
      "K-Nearest Neighbor Classifier done\n",
      "Support Vector Machine Classifier done\n",
      "Multinomial Naive Bayes classifier done\n",
      "Logistic Regression Classifier done\n",
      "[0.8870467429671622, 0.9368916461653652, 0.9132527543052733, 0.9413841052518986]\n"
     ]
    }
   ],
   "source": [
    "# Fold 3\n",
    "print(\"Fold 3\")\n",
    "fold3= []\n",
    "X_train3 = np.concatenate((X[:18698], X[28047:]), axis=0)\n",
    "Y_train3 = np.concatenate((Y[:18698], Y[28047:]), axis=0)\n",
    "X_val3 = X[18698:28047]\n",
    "Y_val3 = Y[18698:28047]\n",
    "\n",
    "fold3 = Machine_Learning_Models(X_train3, Y_train3, X_val3, Y_val3,1)\n",
    "print(fold3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4\n",
      "K-Nearest Neighbor Classifier done\n",
      "Support Vector Machine Classifier done\n",
      "Multinomial Naive Bayes classifier done\n",
      "Logistic Regression Classifier done\n",
      "[0.8950689913359717, 0.9396726922665526, 0.9225585624130923, 0.9461974542731842]\n"
     ]
    }
   ],
   "source": [
    "# Fold 4\n",
    "print(\"Fold 4\")\n",
    "fold4= []\n",
    "X_train4 = np.concatenate((X[:28047], X[37396:]), axis=0)\n",
    "Y_train4 = np.concatenate((Y[:28047], Y[37396:]), axis=0)\n",
    "X_val4 = X[28047:37396]\n",
    "Y_val4 = Y[28047:37396]\n",
    "\n",
    "fold4 = Machine_Learning_Models(X_train4, Y_train4, X_val4, Y_val4,1)\n",
    "print(fold4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5\n",
      "K-Nearest Neighbor Classifier done\n",
      "Support Vector Machine Classifier done\n",
      "Multinomial Naive Bayes classifier done\n",
      "Logistic Regression Classifier done\n",
      "[0.8938923949085463, 0.9435233714835811, 0.9242699753984384, 0.9472670873890255]\n"
     ]
    }
   ],
   "source": [
    "# Fold 5\n",
    "print(\"Fold 5\")\n",
    "X_train5 = X[:37396]\n",
    "Y_train5 = Y[:37396]\n",
    "X_val5 = X[37396:]\n",
    "Y_val5 = Y[37396:]\n",
    "\n",
    "fold5 = Machine_Learning_Models(X_train5, Y_train5, X_val5, Y_val5,1)\n",
    "print(fold5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training accuracy of KNN: 0.8758797732377793\n",
      "Average Training accuracy of SVM: 0.940485613434592\n",
      "Average Training accuracy of Naive Bayes: 0.9192426997539844\n",
      "Average Training accuracy of Logistic Regression: 0.9444646486255215\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Training accuracy of KNN:\", (fold1[0]+fold2[0]+fold3[0]+fold4[0]+fold5[0])/5)\n",
    "print(\"Average Training accuracy of SVM:\", (fold1[1]+fold2[1]+fold3[1]+fold4[1]+fold5[1])/5)\n",
    "print(\"Average Training accuracy of Naive Bayes:\", (fold1[2]+fold2[2]+fold3[2]+fold4[2]+fold5[2])/5)\n",
    "print(\"Average Training accuracy of Logistic Regression:\", (fold1[3]+fold2[3]+fold3[3]+fold4[3]+fold5[3])/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset taken to calculate Test accuracy\n",
    "X_test = tweets_test_dataset['tweet']\n",
    "y_test = tweets_test_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbor Classifier :-\n",
      "Confusion Matrix\n",
      "[[5460  279]\n",
      " [ 776 3485]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91      5739\n",
      "           1       0.93      0.82      0.87      4261\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.90      0.88      0.89     10000\n",
      "weighted avg       0.90      0.89      0.89     10000\n",
      "\n",
      "Support Vector Machine Classifier :-\n",
      "Confusion Matrix\n",
      "[[5596  143]\n",
      " [ 430 3831]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95      5739\n",
      "           1       0.96      0.90      0.93      4261\n",
      "\n",
      "    accuracy                           0.94     10000\n",
      "   macro avg       0.95      0.94      0.94     10000\n",
      "weighted avg       0.94      0.94      0.94     10000\n",
      "\n",
      "Multinomial Naive Bayes classifier :-\n",
      "Confusion Matrix\n",
      "[[5228  511]\n",
      " [ 339 3922]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92      5739\n",
      "           1       0.88      0.92      0.90      4261\n",
      "\n",
      "    accuracy                           0.92     10000\n",
      "   macro avg       0.91      0.92      0.91     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n",
      "Logistic Regression Classifier :-\n",
      "Confusion Matrix\n",
      "[[5586  153]\n",
      " [ 384 3877]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95      5739\n",
      "           1       0.96      0.91      0.94      4261\n",
      "\n",
      "    accuracy                           0.95     10000\n",
      "   macro avg       0.95      0.94      0.94     10000\n",
      "weighted avg       0.95      0.95      0.95     10000\n",
      "\n",
      "Test Accuracy of Models:-\n",
      "KNN :  0.8945\n",
      "SVM :  0.9427\n",
      "Naive Bayes :  0.915\n",
      "Logistic Regression :  0.9463\n"
     ]
    }
   ],
   "source": [
    "Machine_Learning_Models(X,Y,X_test,y_test,0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
